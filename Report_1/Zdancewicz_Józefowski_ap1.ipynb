{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Analiza Przeżycia\"\n",
    "subtitle: \"Raport 1\"\n",
    "authors: \"Jakub Zdancewicz, Ksawery Józefowski\"\n",
    "date: last-modified\n",
    "date-format: \"[Wygenerowano] D [października] YYYY\"\n",
    "format: pdf\n",
    "crossref:  # recznie zmieniamy tytuly tabel itp. na polskie nazwy\n",
    "  fig-title: \"Rysunek\"\n",
    "  tbl-title: \"Tabela\"\n",
    "  # eq-title: \"Równanie\"\n",
    "  # lof-title: \"Spis rysunków\"\n",
    "  # lot-title: \"Spis tabel\"\n",
    "  # lst-title: \"Listing\"\n",
    "# lang: pl  # jezyk polski psuje \"cross references\" oraz formatowanie w tabeli\n",
    "# toc: true  # table of contents\n",
    "execute:\n",
    "  echo: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstęp\n",
    "Sprawozdanie jest podzielone na cztery części.  \n",
    "\n",
    "Część pierwsza dotyczy analizy rozkładu **Exponentiated Weibull**($\\mathcal{E}\\mathcal{W}$) z parametrami $\\alpha$, $\\beta$ oraz $\\gamma$. Obejmuje implementację podstawowych funkcji rozkładu, wizualizację funkcji hazardu, generowanie danych losowych oraz analizę statystyczną wygenerowanych prób.\n",
    "\n",
    "Część druga poświęcona jest analizie danych **cenzurowanych** z uogólnionego rozkładu wykładniczego ($\\mathcal{G}\\mathcal{E}(\\lambda,\\alpha)$). Zadania obejmują generowanie zmiennych losowych cenzurowanych różnych typów, obliczanie rozsądnych statystyk opisowych oraz analizę przykładowych danych eksperymentalnych z obserwacją remisji choroby w dwóch grupach pacjentów.\n",
    "\n",
    "W części trzeciej wyznaczymy oszacowania największej wiarygodności oraz realizacje przedziałów ufności dla średniego czasu remisji choroby w dwóch grupach pacjentów rozważanych w części drugiej. Następnie powtórzymy obliczenia, zakładając, że obserwacje prowadzono tylko do momentu uzyskania remisji u dziesięciu pacjentów. Na koniec porównamy dwa punktowe estymatory parametru rozkładu wykładniczego na podstawie danych cenzurowanych, przeprowadzając symulację oraz oceniając ich obciążenie i błąd średniokwadratowy na wygenerowanych danych.\n",
    "\n",
    "Część czwarta sprawozdania poświęcona jest zagadnieniu weryfikacji hipotez dla danych cenzurowanych I-go typu, pochodzących z rozkładu wykładniczego. W szczególności skonstruujemy i zaimplementujemy test ilorazu wiarygodności dla hipotez dwustronnych oraz jednostronnych (lewostronnych i prawostronnych) dotyczących parametru $\\vartheta$. Następnie przeprowadzimy symulacje mające na celu ocenę własności tego testu, w szczególności jego rozmiaru oraz mocy dla wybranych alternatyw. Opracowany test zastosujemy następnie do weryfikacji hipotezy o średnim czasie remisji choroby na podstawie danych eksperymentalnych w dwóch rozważanych grupach pacjentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import expon, gamma, binomtest\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracje funkcji rozkładu $\\mathcal{E}\\mathcal{W}(\\alpha, \\beta, \\gamma)$\n",
    "\n",
    "Rozkład Exponentiated Weibull ($\\mathcal{E}\\mathcal{W}$) ma trzy parametry $\\alpha > 0$, $\\beta > 0$ oraz $\\gamma > 0$. Definiujemy następujące funkcje:\n",
    "\n",
    "- **Gęstość prawdopodobieństwa:**\n",
    "$$\n",
    "f(t;\\alpha,\\beta,\\gamma) =\n",
    "\\frac{\\alpha \\, \\gamma}{\\beta} \n",
    "\\left(\\frac{t}{\\beta}\\right)^{\\alpha-1} \n",
    "\\left[ 1 - \\exp\\left(-\\left(\\frac{t}{\\beta}\\right)^\\alpha\\right) \\right]^{\\gamma-1} \n",
    "\\exp\\left[-\\left(\\frac{t}{\\beta}\\right)^\\alpha\\right]\n",
    "\\, \\mathbf{1}_{(0, \\infty)}(t),\n",
    "$$\n",
    "- **Dystrybuanta:**\n",
    "$$\n",
    "F(t;\\alpha,\\beta,\\gamma)\n",
    "= \n",
    "\\left[\n",
    "1 - \\exp\\!\\left(-\\left(\\frac{t}{\\beta}\\right)^{\\alpha}\\right)\n",
    "\\right]^{\\gamma}, \\ t \\geq 0\n",
    "$$\n",
    "\n",
    "- **Dystrybuanta odwrotna:**\n",
    "$$\n",
    "Q(p;\\alpha,\\beta,\\gamma)\n",
    "= \n",
    "\\beta \\left[ - \\ln\\left(1-p^{\\frac{1}{\\gamma}}\\right)\\right]^{\\frac{1}{\\alpha}}, \\ p \\in (0,1)\n",
    "$$\n",
    "\n",
    "- **Funkcja hazardu:**\n",
    "$$\n",
    "h(t;\\alpha,\\beta,\\gamma)\n",
    "= \\frac{f(t;\\alpha,\\beta,\\gamma)}{1 - F(t;\\alpha,\\beta,\\gamma))}\n",
    "$$\n",
    "\n",
    "Poniżej podajemy implementacje podanych funkcji w języku Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gęstość prawdopodobieństwa\n",
    "def f(x, alpha, beta, gamma):\n",
    "    if x > 0:\n",
    "        return ((alpha * gamma) / beta) * \\\n",
    "               ((x / beta)**(alpha - 1)) * \\\n",
    "               ((1 - math.exp(-(x / beta)**alpha))**(gamma - 1)) * \\\n",
    "               math.exp(-((x / beta)**alpha))\n",
    "    return 0    \n",
    "    \n",
    "# Dystrybuanta\n",
    "def F(x, alpha, beta, gamma):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    return (1 - math.exp(-(x / beta)**alpha))**gamma\n",
    "    \n",
    "# Dystrybuanta odwrotna\n",
    "def F_inv(p, alpha, beta, gamma):\n",
    "    if not 0 < p < 1:a\n",
    "        raise ValueError(\"p musi być w (0,1)\")\n",
    "    return beta * (-math.log(1 - p**(1/gamma)))**(1/alpha)\n",
    "    \n",
    "# Funkcja hazardu\n",
    "def h(t, alpha, beta, gamma):\n",
    "    return f(t, alpha, beta, gamma) / (1 - F(t, alpha, beta, gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykresy funkcji hazardu dla różnych parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy wykresy funkcji hazardu [-@fig-hazard] dla zestawów parametrów\n",
    "$\\alpha$, $\\beta$, $\\gamma$ przedstawionych w tabeli [-@tbl-params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  $\\alpha$    $\\beta$    $\\gamma$  Typ wykresu\n",
       "----------  ---------  ----------  -------------\n",
       "       1          1          1     stały\n",
       "       0.5        1          0.5   malejący\n",
       "       2          1          2     rosnący\n",
       "       2          1          0.25  wannowy\n",
       "       0.9        0.5        1.3   jednomodalny"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| label: tbl-params\n",
    "params = [\n",
    "    (1, 1, 1, \"α=1, β=1, γ=1 (stały)\"),\n",
    "    (0.5, 1, 0.5, \"α=0.5, β=1, γ=0.5 (malejący)\"),\n",
    "    (2, 1, 2, \"α=2, β=1, γ=2 (rosnący)\"),\n",
    "    (2, 1, 0.25, \"α=2, β=1, γ=0.25 (wannowy)\"),\n",
    "    (0.9, 0.5, 1.3, \"α=0.9, β=0.5, γ=1.3 (jednomodalny)\")\n",
    "]\n",
    "\n",
    "params_dicts = []\n",
    "\n",
    "for p in params:\n",
    "    params_dicts.append({\n",
    "        'alpha': p[0],\n",
    "        'beta': p[1],\n",
    "        'gamma': p[2],\n",
    "        'Typ wykresu': re.search(r'\\((.*?)\\)', p[3]).group(1)\n",
    "    })\n",
    "params_df = pd.DataFrame(params_dicts)\n",
    "params_df.columns = [r'$\\alpha$', r'$\\beta$', r'$\\gamma$', 'Typ wykresu']\n",
    "\n",
    "table_latex = tabulate(\n",
    "    params_df,\n",
    "    headers='keys',\n",
    "    tablefmt='latex_booktab',\n",
    "    showindex=False,\n",
    ")\n",
    "\n",
    "Markdown(table_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "h_vec = np.vectorize(h)\n",
    "\n",
    "x = np.linspace(0.05, 2, 1000)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for alpha, beta, gamma, label in params:\n",
    "    hazard = h_vec(x, alpha, beta, gamma)\n",
    "    plt.plot(x, hazard, label=label)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$h(x)$')\n",
    "plt.title(r'Funkcja hazardu dla rozkładu $\\mathcal{E}\\mathcal{W}$')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('hazard_plot.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wykresy funkcji hazardu dla różnych parametrów rozkładu $\\mathcal{E}\\mathcal{W}$](hazard_plot.pdf){#fig-hazard}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generowanie zmiennych z rozkładu $\\mathcal{E}\\mathcal{W}$\n",
    "Do wygenerowania próbek z rozkładu ($\\mathcal{E}\\mathcal{W}$) stosujemy metodę odwrotnej dystrybuanty.  \n",
    "Niech $U \\sim \\mathcal{U}(0,1)$, wówczas zmienna losowa\n",
    "$$\n",
    "X = F^{-1}(U;\\alpha,\\beta,\\gamma)\n",
    "$$\n",
    "gdzie $F$ jest dystrybuantą rozkładu $\\mathcal{E}\\mathcal{W}$ ma rozkład $\\mathcal{E}\\mathcal{W}(\\alpha,\\beta,\\gamma)$.\n",
    "\n",
    "Poniżej przedstawiamy implementację metody odwrotnej dystrybuanty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie zmiennych losowych EW\n",
    "def generate_EW(n, alpha, beta, gamma):\n",
    "    u = np.random.uniform(0, 1, n)\n",
    "    F_inv_vec = np.vectorize(F_inv)\n",
    "    return F_inv_vec(u, alpha, beta, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogramy i gęstości dla wybranych parametrów\n",
    "Generujemy dane z rozkładu $\\mathcal{E}\\mathcal{W}$ dla następujących zestawów parametrów:\n",
    "$$\n",
    "(\\alpha, \\beta, \\gamma) \\in \\{(2, 1, 2), (2, 1, 0.25)\\},\n",
    "$$\n",
    "oraz dla dwóch rozmiarów prób: $(n = 50)$ i $(n = 100)$.\n",
    "\n",
    "Dla każdej kombinacji tworzymy histogramy z nałożoną **gęstością teoretyczną**, aby porównać kształt histogramów z kształtem wykresu rozkładu teoretycznego ([-@fig-histograms])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "param_sets = [(2, 1, 2, \"α=2, β=1, γ=2\"),\n",
    "              (2, 1, 0.25, \"α=2, β=1, γ=0.25\")]\n",
    "sample_sizes = [50, 100]\n",
    "np.random.seed(123456)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "f_vec = np.vectorize(f)\n",
    "\n",
    "samples = {}\n",
    "\n",
    "for i, (alpha, beta, gamma, label) in enumerate(param_sets):\n",
    "    for j, n in enumerate(sample_sizes):\n",
    "        data = generate_EW(n, alpha, beta, gamma)\n",
    "        samples[(alpha, beta, gamma, n)] = data\n",
    "        axes[i, j].hist(data, bins=20, density=True, alpha=0.6,\n",
    "                        label='Histogram')\n",
    "        \n",
    "        x = np.linspace(0.05, max(data), 100)\n",
    "        y = f_vec(x, alpha, beta, gamma)\n",
    "        axes[i, j].plot(x, y, 'r-', label='Gęstość teoretyczna')\n",
    "        \n",
    "        axes[i, j].set_xlabel('$x$')\n",
    "        axes[i, j].set_ylabel('Gęstość')\n",
    "        axes[i, j].set_title(f'{label}, n={n}')\n",
    "        axes[i, j].legend()\n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('histograms.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogramy i gęstości teoretyczne dla rozkładu $\\mathcal{E}\\mathcal{W}$](histograms.pdf){#fig-histograms}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że kształty histogramów są zbliżone do wykresów gęstości teoretycznych, co jest szczególnie widoczne przy większej liczbie obserwacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statystyki opisowe wygenerowanych próbek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla wygenerowanych próbek obliczamy podstawowe statystyki opisowe: średnią, medianę, odchylenie standardowe, pierwszy kwartyl (Q1), trzeci kwartyl (Q3), rozstęp między kwartylowy, minimum oraz maksimum. \n",
    "\n",
    "Dodatkowo podajemy wartości teoretyczne mediany oraz kwartylów Q1 i Q3, obliczone za pomocą funkcji kwantylowej $F^{-1}(p;\\alpha,\\beta,\\gamma)$, aby umożliwić porównanie wyników empirycznych z wartościami teoretycznymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "quantiles_theo = []\n",
    "quantiles_emp = []\n",
    "stats = []\n",
    "\n",
    "for (alpha, beta, gamma, n), data in samples.items():\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    min_val, max_val = np.min(data), np.max(data)\n",
    "    range_val = max_val - min_val\n",
    "\n",
    "    theo_median = F_inv(0.5, alpha, beta, gamma)\n",
    "    theo_q1 =F_inv(0.25, alpha, beta, gamma)\n",
    "    theo_q3 = F_inv(0.75, alpha, beta, gamma)\n",
    "    if n != 100:\n",
    "        quantiles_theo.append({\n",
    "            'alpha': alpha, 'beta': beta, 'gamma': gamma,\n",
    "            'Mediana_Teor': theo_median,\n",
    "            'Q1_Teor': theo_q1,\n",
    "            'Q3_Teor': theo_q3,\n",
    "        })\n",
    "\n",
    "    quantiles_emp.append({\n",
    "        'alpha': alpha, 'beta': beta, 'gamma': gamma, 'n': int(n),\n",
    "        'Mediana_Emp': median,\n",
    "        'Q1_Emp': q1,\n",
    "        'Q3_Emp': q3,\n",
    "    })\n",
    "    stats.append({\n",
    "        'alpha': alpha, 'beta': beta, 'gamma': gamma, 'n': int(n),\n",
    "        'Średnia': mean,\n",
    "        'Odch. std.': std,\n",
    "        'Min': min_val, 'Maks': max_val, 'Rozstęp': range_val\n",
    "    })\n",
    "\n",
    "quantiles_theo_df = pd.DataFrame(quantiles_theo)\n",
    "quantiles_emp_df = pd.DataFrame(quantiles_emp)\n",
    "stats_df = pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statystyki teoretyczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela [-@tbl-quantile-theo] przedstawia wartości teoretyczne mediany oraz kwartylów Q1 i Q3 dla dwóch rozmiarów prób i wybranych zestawów parametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  $\\alpha$    $\\beta$    $\\gamma$    $Mediana_{teor.}$    $Q1_{teor.}$    $Q3_{teor.}$\n",
       "----------  ---------  ----------  -------------------  --------------  --------------\n",
       "         2          1        2                1.10813        0.832555         1.41778\n",
       "         2          1        0.25             0.254044       0.0625612        0.616759"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Wartości teoretyczne mediany i kwartylów Q1 oraz Q3 dla wygenerowanych próbek.\n",
    "#| label: tbl-quantile-theo\n",
    "#| echo: false\n",
    "quantiles_theo_df.columns = [r'$\\alpha$', r'$\\beta$', r'$\\gamma$', r'$Mediana_{teor.}$', r'$Q1_{teor.}$', r'$Q3_{teor.}$']\n",
    "\n",
    "table_quantiles_theo = tabulate(\n",
    "    quantiles_theo_df,\n",
    "    headers='keys',\n",
    "    tablefmt='latex_booktab',\n",
    "    showindex=False,\n",
    ")\n",
    "\n",
    "Markdown(table_quantiles_theo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statystyki empiryczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela [-@tbl-quantile-emp] przedstawia wartości empiryczne mediany oraz kwartylów Q1 i Q3 obliczone na podstawie wygenerowanych próbek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  $\\alpha$    $\\beta$    $\\gamma$    n    $Mediana_{emp.}$    $Q1_{emp.}$    $Q3_{emp.}$\n",
       "----------  ---------  ----------  ---  ------------------  -------------  -------------\n",
       "         2          1        2      50            1.01077       0.831474        1.32946\n",
       "         2          1        2     100            1.12231       0.853751        1.39262\n",
       "         2          1        0.25   50            0.314101      0.083986        0.54074\n",
       "         2          1        0.25  100            0.270129      0.0815788       0.540099"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Wartości empiryczne mediany i kwartylów Q1 oraz Q3 dla wygenerowanych próbek.\n",
    "#| label: tbl-quantile-emp\n",
    "#| echo: false\n",
    "quantiles_emp_df.columns = [r'$\\alpha$', r'$\\beta$', r'$\\gamma$', 'n', r'$Mediana_{emp.}$', r'$Q1_{emp.}$', r'$Q3_{emp.}$']\n",
    "\n",
    "table_quantiles_emp = tabulate(\n",
    "   quantiles_emp_df,\n",
    "    headers='keys',\n",
    "    tablefmt='latex_booktab',\n",
    "    showindex=False,\n",
    ")\n",
    "\n",
    "Markdown(table_quantiles_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że wartości statystyk empirycznych są zbliżone do wartości statystyk teoretycznych, zwłaszcza w przypadku większych wartości $n$, co jest zgodne z intuicją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatkowe statystyki opisowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela [-@tbl-stats] przedstawia dodatkowe statystyki opisowe: średnią, odchylenie standardowe, minimum, maksimum oraz rozstęp między kwartylowy dla tych samych próbek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  $\\alpha$    $\\beta$    $\\gamma$    n    Średnia    Odch. std.         Min.    Maks.    Rozstęp\n",
       "----------  ---------  ----------  ---  ---------  ------------  -----------  -------  ---------\n",
       "         2          1        2      50   1.11529       0.390867  0.464871     2.20705    1.74218\n",
       "         2          1        2     100   1.15452       0.43663   0.371057     2.38142    2.01036\n",
       "         2          1        0.25   50   0.446857      0.470365  0.000371554  1.79839    1.79801\n",
       "         2          1        0.25  100   0.397005      0.425948  5.34687e-05  1.84392    1.84387"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Dodatkowe statystyki opisowe dla wygenerowanych próbek.\n",
    "#| label: tbl-stats\n",
    "#| echo: false\n",
    "stats_df.columns = [r'$\\alpha$', r'$\\beta$', r'$\\gamma$', 'n', 'Średnia', 'Odch. std.', 'Min.',  'Maks.',  'Rozstęp']\n",
    "\n",
    "table_stats = tabulate(\n",
    "    stats_df,\n",
    "    headers='keys',\n",
    "    tablefmt='latex_booktab',\n",
    "    showindex=False,\n",
    ")\n",
    "\n",
    "Markdown(table_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że w obu przypadkach wartości średnie są wyższe od wartości mediany, co może wskazywać na prawostronną asymetrię rozkładów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generowanie zmiennych cenzurowanych z uogólnionego rozkładu wykładniczego ($\\mathcal{G}\\mathcal{E}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie próbek losowych z rozkładu ($\\mathcal{G}\\mathcal{E}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W pierwszym kroku zdefiniujmy funkcję odwrotnej dystrybuanty dla rozkładu $\\mathcal{GE}(\\lambda, \\alpha)$. Umożliwi nam ona generowanie próbek z tego rozkładu metodą odwrotnej dystrybuanty.\n",
    "$$\n",
    "F^{-1}(u;\\lambda,\\alpha) = -\\frac{1}{\\lambda} \\ln\\Bigl(1 - u^{\\frac{1}{\\alpha}}\\Bigr), \\quad u \\in (0,1).\n",
    "$$\n",
    "\n",
    "Poniżej przedstawiamy implementację metody odwrotnej dystrybuanty dla tego rozkładu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GE(n, L, alpha):\n",
    "    # generujemy liczby z U(0,1)\n",
    "    u = np.random.uniform(0, 1, n)\n",
    "    # funkcja odwrotnej dystrybuanty\n",
    "    return -1/L * np.log(1 - u**(1/alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenzurowanie danych z rozkładu $\\mathcal{GE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W kolejnych krokach definiujemy trzy algorytmy do generowania danych cenzurowanych z rozkładu $\\mathcal{GE}(\\lambda, \\alpha)$, odpowiadające różnym przypadkom prawostronnego cenzurowania:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Typ I**:\n",
    "Ten typ cenzurowania charakteryzuje się ustalonym czasem trwania eksperymentu $t_0$.\n",
    "\n",
    "**Algorytm:**\n",
    "\n",
    "- Generujemy próbę $X \\sim \\mathcal{GE}(\\lambda, \\alpha)$.\n",
    "\n",
    "-  Tworzymy wektor $\\delta$, w którym `True` oznacza obserwacje niecenzurowane\n",
    " $(X_s \\le t_0)$, a `False` obserwacje cenzurowane $(X_s > t_0)$.\n",
    "\n",
    "-  Dane obserwowane, dla których $X_s > t_0$, przycinamy do wartości $t_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_I(t0, L, alpha, n):\n",
    "    X_nc = GE(n, L, alpha)\n",
    "    delta = (X_nc <= t0).astype(bool)\n",
    "    X_c = np.minimum(X_nc, t0)\n",
    "    return pd.DataFrame({\"Data\": X_c, \"Delta\": delta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Typ II**:\n",
    "Ten typ cenzurowania charakteryzuje się ustaloną liczbą zaobserwowanych zdarzeń $m$.\n",
    "\n",
    "**Algorytm:**\n",
    "\n",
    "   - Sortujemy wygenerowane próbki $X \\sim \\mathcal{GE}(\\lambda, \\alpha)$ w kolejności rosnącej.\n",
    "\n",
    "   - Pierwsze $m$ najmniejszych wartości traktujemy jako dane niecenzurowane (`delta=True`).\n",
    "\n",
    "   - Pozostałe obserwacje przycinamy do wartości $m$-tej próbki (`delta=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_II(m, L, alpha, n):\n",
    "    X_nc = np.sort(GE(n, L, alpha))\n",
    "    t_m = X_nc[m-1]\n",
    "    delta = np.zeros(n, dtype=bool)\n",
    "    delta[:m] = True\n",
    "    X_c = np.minimum(X_nc, t_m)\n",
    "    return pd.DataFrame({\"Data\": X_c, \"Delta\": delta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Typ Losowy**:\n",
    "Ten typ cenzurowania charakteryzuje się tym, że czas cenzurowania jest generowany losowo i niezależnie dla każdej obserwacji.\n",
    "\n",
    "**Algorytm:**\n",
    "\n",
    "   - Generujemy próbkę $X \\sim \\mathcal{GE}(\\lambda, \\alpha)$.\n",
    "\n",
    "   - Dla każdej obserwacji generujemy niezależny czas cenzurowania $C_i \\sim \\mathrm{Exp}(\\eta)$.\n",
    "\n",
    "   - Obserwowane dane przycinamy do wartości cenzury (jeśli są większe) $C_i:$ $X_i^{\\mathrm{obs}} = \\min(X_i, C_i)$.\n",
    "\n",
    "   - Tworzymy wektor $\\delta$, w którym `True` oznacza obserwacje niecenzurowane $(X_i \\le C_i)$, a `False` obserwacje cenzurowane $(X_i > C_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_random(eta, L, alpha, n):\n",
    "    C_i = GE(n, eta, 1)\n",
    "    X_nc = GE(n, L, alpha)\n",
    "    X_c = np.minimum(X_nc, C_i)\n",
    "    delta = X_c != C_i\n",
    "    return pd.DataFrame({\"Data\": X_c, \"Delta\": delta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie i analiza danych cenzurowanych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z wcześniej zdefiniowanych funkcji, generujemy po jednym zbiorze danych cenzurowanych dla każdego z trzech typów cenzurowania.  \n",
    "\n",
    "Dla zapewnienia powtarzalności wyników ustalamy ziarno generatora liczb losowych `np.random.seed(37)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37)\n",
    "# Generowanie danych cenzurowanych typu I\n",
    "df_type_I = type_I(t0=1, L=1, alpha=1, n=10000)\n",
    "# Generowanie danych cenzurowanych typu II\n",
    "df_type_II = type_II(m=4000, L=1, alpha=1, n=10000)\n",
    "# Generowanie danych cenzurowanych losowo\n",
    "df_type_random = type_random(eta=1, L=1, alpha=1, n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie dla wygenerowanych zbiorów danych tworzymy zestawienie sensownych statystyk opisowych w formie tabel [-@tbl-cenz-1] oraz [-@tbl-cenz-2], które przedstawiają podstawowe miary położenia, rozproszenia oraz liczbę pełnych obserwacji dla każdego typu cenzurowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def stats(df):\n",
    "    full = df[df[\"Delta\"] == True][\"Data\"]\n",
    "    censored = df[df[\"Delta\"] == False][\"Data\"]\n",
    "    return [\n",
    "        len(df),\n",
    "        len(full),\n",
    "        len(censored),\n",
    "        float(f\"{np.min(full):.3f}\"),\n",
    "        float(f\"{np.percentile(full, 25):.3f}\"),\n",
    "        float(f\"{np.median(full):.3f}\"),\n",
    "        float(f\"{np.percentile(full, 75):.3f}\"),\n",
    "        float(f\"{np.max(full):.3f}\"),\n",
    "        float(f\"{np.percentile(full, 75)-np.percentile(full, 25):.3f}\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "                 Liczba danych    Liczba pełnych    Liczba cenz.\n",
       "-------------  ---------------  ----------------  --------------\n",
       "Cenz. I typu             10000              6334            3666\n",
       "Cenz. II typu            10000              4000            6000\n",
       "Cenz. losowe             10000              4997            5003"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Liczba danych dla wszystkich typów cenzurowań\n",
    "#| label: tbl-cenz-1\n",
    "#| echo: false\n",
    "stat_df_1 = pd.DataFrame(\n",
    "    [stats(df_type_I), stats(df_type_II), stats(df_type_random)],\n",
    "    columns=[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\", \"Min\", \"Q1\", \"Mediana\", \"Q3\", \"Max\", \"IQR\"],\n",
    "    index=[\"Cenz. I typu\", \"Cenz. II typu\", \"Cenz. losowe\"]\n",
    ")\n",
    "\n",
    "table1 = stat_df_1[[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\"]]\n",
    "Markdown(tabulate(table1, headers=table1.columns, showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "                 Min     Q1    Mediana     Q3    Max    IQR\n",
       "-------------  -----  -----  ---------  -----  -----  -----\n",
       "Cenz. I typu       0  0.169      0.389  0.642  1      0.473\n",
       "Cenz. II typu      0  0.103      0.221  0.353  0.498  0.25\n",
       "Cenz. losowe       0  0.146      0.347  0.706  4.208  0.56"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Statystyki dla wszystkich typów cenzurowań\n",
    "#| label: tbl-cenz-2\n",
    "#| echo: false\n",
    "\n",
    "table2 = stat_df_1.drop(columns=[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\"])\n",
    "Markdown(tabulate(table2, headers=table2.columns, showindex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zrezygnowaliśmy z obliczania średniej oraz odchylenia standardowego, ponieważ w przypadku danych cenzurowanych statystyki te tracą sensowną interpretację i mogą prowadzić do błędnych wniosków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza czasu do remisji w badaniu klinicznym z cenzurowaniem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozważamy dane pochodzące od $40$ pacjentów, podzielonych losowo na dwie równoliczne grupy po $20$ osób każda. W każdej grupie połowa pacjentów posiada pełne obserwacje czasu do remisji, natomiast pozostała połowa danych jest cenzurowana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupa A\n",
    "full_A = np.array([0.03345514, 0.08656403, 0.08799947, 0.24385821, 0.27755032,\n",
    "                   0.40787247, 0.58825664, 0.64125620, 0.90679161, 0.94222208])\n",
    "censored_A = np.ones(10)  # wartości dla danych cenzurowanych\n",
    "data_A = np.concatenate([full_A, censored_A])\n",
    "delta_A = np.array([True]*len(full_A) + [False]*len(censored_A))\n",
    "\n",
    "# Grupa B\n",
    "full_B = np.array([0.03788958, 0.12207257, 0.20319983, 0.24474299, 0.30492413,\n",
    "                   0.34224462, 0.42950144, 0.44484582, 0.63805066, 0.69119721])\n",
    "censored_B = np.ones(10)  # wartości dla danych cenzurowanych\n",
    "data_B = np.concatenate([full_B, censored_B])\n",
    "delta_B = np.array([True]*len(full_B) + [False]*len(censored_B))\n",
    "\n",
    "df_A = pd.DataFrame({\"Data\": data_A, \"Delta\": delta_A})\n",
    "df_B = pd.DataFrame({\"Data\": data_B, \"Delta\": delta_B})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla obu grup generujemy zestawienia statystyk opisowych w formie tabel [-@tbl-lek-1] oraz [-@tbl-lek-2], które pozwalają zobrazować podstawowe właściwości rozkładu czasów remisji oraz stopień cenzurowania danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "           Liczba danych    Liczba pełnych    Liczba cenz.\n",
       "-------  ---------------  ----------------  --------------\n",
       "Grupa A               20                10              10\n",
       "Grupa B               20                10              10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Liczba danych dla obu grup\n",
    "#| label: tbl-lek-1\n",
    "#| echo: false\n",
    "stat_df_2 = pd.DataFrame(\n",
    "    [stats(df_A), stats(df_B)],\n",
    "    columns=[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\", \"Min\", \"Q1\", \"Mediana\", \"Q3\", \"Max\", \"IQR\"],\n",
    "    index=[\"Grupa A\", \"Grupa B\"]\n",
    ")\n",
    "\n",
    "table3 = stat_df_2[[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\"]]\n",
    "Markdown(tabulate(table3, headers=table3.columns, showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "           Min     Q1    Mediana     Q3    Max    IQR\n",
       "-------  -----  -----  ---------  -----  -----  -----\n",
       "Grupa A  0.033  0.127      0.343  0.628  0.942  0.501\n",
       "Grupa B  0.038  0.214      0.324  0.441  0.691  0.227"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Statystyki dla grupy A i grupy B\n",
    "#| label: tbl-lek-2\n",
    "#| echo: false\n",
    "\n",
    "table4 = stat_df_2.drop(columns=[\"Liczba danych\", \"Liczba pełnych\", \"Liczba cenz.\"])\n",
    "Markdown(tabulate(table4, headers=table4.columns, showindex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretacja wyników"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W obu grupach liczba pełnych obserwacji była taka sama (10 na 20 pacjentów), jednak rozkład czasów remisji różni się między lekami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupa A (lek A) charakteryzuje się:\n",
    "\n",
    "- większą zmiennością czasów remisji (szerszy rozstęp międzykwartylowy)\n",
    "\n",
    "- szerszym zakresem obserwowanych wartości (niższe minimum i wyższe maksimum)\n",
    "\n",
    "- wyższą medianą, co sugeruje dłuższy typowy czas remisji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupa B (lek B) natomiast:\n",
    "\n",
    "- wykazuje mniejszą zmienność czasów remisji (bardziej skupione obserwacje)\n",
    "  \n",
    "- ma mniejszy rozrzut wartości\n",
    "\n",
    "- cechuje się niższą medianą, co sugeruje szybszą przeciętną reakcję na leczenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lek B wydaje się działać szybciej i w sposób bardziej przewidywalny, prowadząc do remisji w średnio krótszym czasie. Należy jednak pamiętać, że dane są cenzurowane, a wielkość próby niewielka, dlatego nie można wyciągać jednoznacznych wniosków co do skuteczności leków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estymacja średniego czasu remisji choroby dla leków A i B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem jest oszacowanie średniego czasu do remisji choroby u pacjentów leczonych lekami A oraz B. Ponieważ obserwację zakończono po roku, w tym samym ustalonym momencie $t_0 = 1$, mamy do czynienia z danymi cenzurowanymi prawostronnie I-go typu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakładamy, że czas do remisji $X_i$ ma rozkład wykładniczy o gęstości\n",
    "$$\n",
    "f(x) = \\vartheta e^{-\\vartheta x}, \\quad x > 0, \\ \\vartheta > 0\n",
    "$$\n",
    "Wtedy średni czas do remisji wynosi\n",
    "$$\n",
    "\\mu = \\frac{1}{\\vartheta}\n",
    "$$\n",
    "\n",
    "Wyznaczymy punktowe oszacowania największej wiarogodności, oszacowania przy pomocy prostszego estymatora opartego na zmiennej losowej $R$ oznaczającej liczbę obserwacji niecenzurowanych, oraz realizację zbioru ufności opartą na tym uproszczonym estymatorze punktowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estymator największej wiarogodności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla danych prawostronnie cenzurowanych I-go typu funkcja wiarogodności jest dana wzorem\n",
    "$$\n",
    "L(\\vartheta; t^*) = \\frac{n!}{(n-r)!} \\prod_{i=1}^{r} f_\\vartheta(x_{(i)}) \\, [1 - F_\\vartheta(t_0)]^{n-r},\n",
    "$$\n",
    "gdzie $x_{(1)} < x_{(2)} < \\dots < x_{(r)}$, $f_\\vartheta$ to gęstość, $F_\\vartheta$ – dystrybuanta rozkładu zmiennych $X_1, \\dots, X_n$, a $r$ jest realizacją zmiennej losowej $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estymatorem największej wiarogodności (NW) parametru $\\vartheta$ jest wtedy statystyka postaci\n",
    "$$\n",
    "\\hat{\\vartheta} = \\frac{R}{T_1},\n",
    "$$\n",
    "gdzie\n",
    "$$\n",
    "T_1 = \\sum_{i=1}^{R} X_{(i)} + t_0 (n - R)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej przedstawiamy kod oraz estymację parametru $\\mu = \\frac{1}{\\vartheta}$ na podstawie posiadanych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lek A: Średnia = 1.4215826169999999\n",
      "Lek B: Średnia = 1.3458668850000002\n"
     ]
    }
   ],
   "source": [
    "r = 10 # liczba remisji\n",
    "n = 20 # liczba pacjentów\n",
    "t0 = 1 # czas cenzurowania\n",
    "\n",
    "# Suma czasów remisji dla leków A i B\n",
    "sum_A = full_A.sum()\n",
    "sum_B = full_B.sum()\n",
    "\n",
    "# Estymacja NW\n",
    "T_A = sum_A + t0 * (n - r)\n",
    "theta_A = r / T_A\n",
    "mu_A = 1 / theta_A\n",
    "\n",
    "T_B = sum_B + t0 * (n - r)\n",
    "theta_B = r / T_B\n",
    "mu_B = 1 / theta_B\n",
    "\n",
    "print(f\"Lek A: Średnia = {mu_A}\")\n",
    "print(f\"Lek B: Średnia = {mu_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatywny estymator punktowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostszy estymator parametru $\\vartheta$ można uzyskać, korzystając jedynie ze zmiennej losowej $R$.  \n",
    "Ponieważ $R \\sim \\mathcal{B}(n, F_\\vartheta(t_0))$, prawdopodobieństwo sukcesu wynosi\n",
    "$$\n",
    "p = 1 - \\exp(-\\vartheta t_0).\n",
    "$$\n",
    "Przyjmując za estymator $p$ statystykę\n",
    "$$\n",
    "\\tilde{p} = \\frac{R}{n},\n",
    "$$\n",
    "otrzymujemy alternatywny estymator parametru $\\vartheta$ w postaci\n",
    "$$\n",
    "\\tilde{\\vartheta} = -\\frac{\\log\\!\\left(1 - \\frac{R}{n}\\right)}{t_0}\n",
    "$$\n",
    "Poniżej przedstawiamy kod oraz estymację parametru $\\mu = \\frac{1}{\\vartheta}$ na podstawie posiadanych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lek A: Średnia = 1.4426950408889634\n",
      "Lek B: Średnia = 1.4426950408889634\n"
     ]
    }
   ],
   "source": [
    "r = 10 # liczba remisji\n",
    "n = 20 # liczba pacjentów\n",
    "t0 = 1 # czas cenzurowania\n",
    "\n",
    "theta_A_exp = -np.log(1 - r / n) / t0\n",
    "mu_A_exp = 1 / theta_A_exp\n",
    "\n",
    "theta_B_exp = -np.log(1 - r / n) / t0\n",
    "mu_B_exp = 1 / theta_B_exp\n",
    "\n",
    "print(f\"Lek A: Średnia = {mu_A_exp}\")\n",
    "print(f\"Lek B: Średnia = {mu_B_exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że w tym przypadku estymacje zależą wyłącznie od wartości $r$, $n$ oraz $t_0$, w związku z czym średnia w naszym przykładzie będzie taka sama dla obu leków, wynosząc w przybliżeniu $\\approx 1.44$.  \n",
    "Dlatego zastosowanie tego estymatora w tym kontekście nie ma sensu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estymacja przedziałowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbiór ufności dla parametru $\\vartheta$ można skonstruować w oparciu o estymator\n",
    "$$\n",
    "\\tilde{\\vartheta} = -\\frac{\\log\\!\\left(1 - \\frac{R}{n}\\right)}{t_0},\n",
    "$$\n",
    "który zależy wyłącznie od zmiennej losowej $R$.  \n",
    "\n",
    "Ponieważ $R \\sim \\mathcal{B}(n, F_\\vartheta(t_0))$, mając przedział ufności $[T_L, T_U]$ dla prawdopodobieństwa sukcesu\n",
    "$$\n",
    "p = F_\\vartheta(t_0) = 1 - \\exp(-\\vartheta t_0),\n",
    "$$\n",
    "na poziomie ufności $1 - \\alpha$, można go przekształcić w przedział ufności dla parametru $\\vartheta$:\n",
    "$$\n",
    "[\\tilde{T}_L, \\tilde{T}_U] = \\left[-\\frac{\\ln(1 - T_L)}{t_0}, \\; -\\frac{\\ln(1 - T_U)}{t_0}\\right],\n",
    "$$\n",
    "również na poziomie ufności $1 - \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej przedstawiamy realizację tego przedziału ufności dla parametru $\\mu = \\frac{1}{\\vartheta}$ na podstawie posiadanych danych.  \n",
    "Do obliczeń wykorzystamy przedział ufności dla paramtru $p$ w rozkładzie Bernoulliego, dostępny w pakiecie _scipy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poziom istotności 0.95: [0.7679853394899884, 3.1506350087666912]\n",
      "Poziom istotności 0.99: [0.6559875341960035, 4.072031273887426]\n"
     ]
    }
   ],
   "source": [
    "r = 10  # liczba sukcesów\n",
    "n = 20  # liczba prób\n",
    "alphas = [0.05, 0.01] # poziomy istotności\n",
    "\n",
    "result = binomtest(r, n)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Przedział ufności dla p\n",
    "    confidence_level = 1 - alpha\n",
    "    ci = result.proportion_ci(confidence_level=confidence_level)\n",
    "    # Transformacja do przedziału\n",
    "    T_L = -np.log(1 - ci.low)\n",
    "    T_R = -np.log(1 - ci.high)\n",
    "\n",
    "    print(f\"Poziom istotności {1-alpha}: [{1/T_R}, {1/T_L}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że przedziały ufności są identyczne dla obu leków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oszacowanie parametrów przy cenzurowaniu II-go typu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy teraz, że obserwacje czasu do remisji choroby były prowadzone do momentu, w którym remisja zostanie zaobserwowana u dziesięciu pacjentów.  \n",
    "W takim przypadku mamy do czynienia z danymi cenzurowanymi prawostronnie II-go typu.\n",
    "\n",
    "Ponownie wyznaczymy punktowe oszacowania największej wiarogodności oraz realizację zbioru ufności, tym razem opartą na estymatorze NW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estymator największej wiarogodności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla danych prawostronnie cenzurowanych II-go typu funkcja wiarogodności przyjmuje postać\n",
    "$$\n",
    "L(\\vartheta; t) = \\frac{n!}{(n-m)!} \\prod_{i=1}^{m} f_\\vartheta(x_{(i)}) \\, [1 - F_\\vartheta(x_{(m)})]^{\\,n-m},\n",
    "$$\n",
    "gdzie $0 < x_{(1)} < x_{(2)} < \\dots < x_{(m)} < \\infty$, $f_\\vartheta$ oznacza gęstość, $F_\\vartheta$ - dystrybuantę rozkładu zmiennych $X_1, \\dots, X_n$, a $m$ jest ustalone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wówczas estymatorem największej wiarogodności (NW) parametru $\\vartheta$ jest statystyka\n",
    "$$\n",
    "\\hat{\\vartheta} = \\frac{m}{T_2},\n",
    "$$\n",
    "gdzie\n",
    "$$\n",
    "T_2 = \\sum_{i=1}^{m} X_{(i)} + (n - m) X_{(m)}.\n",
    "$$\n",
    "\n",
    "Poniżej przedstawiamy kod oraz estymację parametru $\\mu = \\frac{1}{\\vartheta}$ na podstawie posiadanych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas do remisji (A): 1.363804697\n",
      "Średni czas do remisji (B): 1.037064095\n"
     ]
    }
   ],
   "source": [
    "# Liczba obserwacji niecenzurowanych\n",
    "m = 10\n",
    "\n",
    "# Estymacja dla leku A\n",
    "T_A = full_A.sum() + m * full_A[-1]\n",
    "theta_A = m / T_A\n",
    "print(\"Średni czas do remisji (A):\", 1 / theta_A)\n",
    "\n",
    "# Estymacja dla leku B\n",
    "T_B = full_B.sum() + m * full_B[-1]\n",
    "theta_B = m / T_B\n",
    "print(\"Średni czas do remisji (B):\", 1 / theta_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estymacja przedziałowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do konstrukcji przedziału ufności dla parametru $\\vartheta$ wykorzystujemy fakt, że zmienna losowa\n",
    "$$\n",
    "\\frac{\\vartheta \\sum_{i=1}^{m} D_i}{m} = \\frac{\\sum_{i=1}^{m} D_i}{m \\mu},\n",
    "$$\n",
    "gdzie \n",
    "$$\n",
    "D_i = (n - i + 1)(X_{(i)} - X_{(i-1)}), \\quad i = 1, \\dots, m,\n",
    "$$\n",
    "ma rozkład gamma $\\mathcal{G}(m, 1/m)$ i może służyć jako funkcja centralna w konstrukcji przedziałów ufności.\n",
    "\n",
    "Niech $q_m(p)$ oznacza kwantyl rzędu $p$ rozkładu gamma $G(m, 1/m)$. Wówczas przedział ufności dla $\\vartheta$ na poziomie ufności $1 - \\alpha$ można zapisać jako\n",
    "$$\n",
    "[T_L, T_U] = \\left[\\frac{m q_m(\\alpha_1)}{\\sum_{i=1}^{m} D_i}, \\;\\frac{m q_m(1-\\alpha_2)}{\\sum_{i=1}^{m} D_i}\\right].\n",
    "$$\n",
    "\n",
    "Poniżej przedstawiamy realizację tego przedziału ufności dla parametru $\\mu = \\frac{1}{\\vartheta}$ na podstawie posiadanych danych.  \n",
    "Do obliczeń wykorzystamy kwantyle rozkładu gamma obliczone przy pomocy pakietu _scipy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przedziały ufności dla leku A:\n",
      "Poziom istotności 0.95: [0.7982560062092574, 2.8439919752489153]\n",
      "Poziom istotności 0.99: [0.6819561154044388, 3.669177477392265]\n",
      "\n",
      "\n",
      "Przedziały ufności dla leku B:\n",
      "Poziom istotności 0.95: [0.6070096726303604, 2.162627809162604]\n",
      "Poziom istotności 0.99: [0.5185729329187228, 2.7901152037066144]\n"
     ]
    }
   ],
   "source": [
    "# Liczba obserwacji niecenzurowanych\n",
    "m = 10\n",
    "\n",
    "# Poziomy istotności\n",
    "alphas = [0.05, 0.01]\n",
    "\n",
    "def ci_gamma(T, m, alpha):\n",
    "    T_L = m * scipy.stats.gamma.ppf(alpha/2, m, scale=1/m) / T\n",
    "    T_U = m * scipy.stats.gamma.ppf(1 - alpha/2, m, scale=1/m) / T\n",
    "    return T_L, T_U\n",
    "\n",
    "groups = {\"A\": T_A, \"B\": T_B}\n",
    "\n",
    "for group, T in groups.items():\n",
    "    print(f\"Przedziały ufności dla leku {group}:\")\n",
    "    for alpha in alphas:\n",
    "        T_L, T_U = ci_gamma(T, m, alpha)\n",
    "        print(f\"Poziom istotności {1-alpha}: [{1/T_U}, {1/T_L}]\")\n",
    "    if (group == \"A\"):\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symulacyjne porównanie estymatorów parametru $\\vartheta$ dla danych prawostronnie cenzurowanych I-go typu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niech $X_1, \\dots, X_n$ będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym $\\mathcal{E}(\\vartheta)$, a $t_0$ ustalonym czasem obserwacji. Naszym celem jest porównanie estymatorów parametru $\\vartheta$ dla danych prawostronnie cenzurowanych I-go typu. Rozważamy estymator największej wiarygodności (NW):\n",
    "$$\n",
    "\\hat{\\vartheta} = \\frac{R}{T_1},\n",
    "$$\n",
    "oraz prosty estymator:\n",
    "$$\n",
    "\\tilde{\\vartheta} = -\\frac{\\log(1 - \\frac R n )}{t_0}.\n",
    "$$\n",
    "\n",
    "W celu porównania dokładności obu estymatorów przeprowadziliśmy symulacje dla parametrów:\n",
    "$$\n",
    "\\vartheta = 1, \\quad n = 10, 30, \\quad t_0 = 0.5, 1, 2,\n",
    "$$\n",
    "z $k = 10000$ powtórzeniami. Dla każdej kombinacji obliczyliśmy średni bias oraz błąd średniokwadratowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parametry symulacji\n",
    "theta = 1\n",
    "n_values = [10, 30]\n",
    "t0_values = [0.5, 1, 2]\n",
    "k = 10000  # liczba powtórzeń\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in n_values:\n",
    "    for t0 in t0_values:\n",
    "        bias_NW = 0\n",
    "        mse_NW = 0\n",
    "        bias_simple = 0\n",
    "        mse_simple = 0\n",
    "\n",
    "        for _ in range(k):\n",
    "            while True:\n",
    "                data = type_I(t0, theta, 1, n)\n",
    "                X_c = data.Data\n",
    "                delta = data.Delta\n",
    "                R = delta.sum()\n",
    "                if R != n:\n",
    "                    break\n",
    "\n",
    "            # Estymator NW\n",
    "            X_c = sorted(X_c)\n",
    "            T1 = sum(X_c[:R]) + t0*(n - R)\n",
    "            theta_NW = R / T1\n",
    "            bias_NW += theta_NW - theta\n",
    "            mse_NW += (theta_NW - theta)**2\n",
    "\n",
    "            # Prosty estymator\n",
    "            theta_simple = -np.log(1 - R / n) / t0\n",
    "            bias_simple += theta_simple - theta\n",
    "            mse_simple += (theta_simple - theta)**2\n",
    "\n",
    "        bias_NW /= k\n",
    "        mse_NW /= k\n",
    "        bias_simple /= k\n",
    "        mse_simple /= k\n",
    "\n",
    "        results.append({\n",
    "            \"n\": int(n),\n",
    "            \"t0\": t0,\n",
    "            \"Bias_NW\": bias_NW,\n",
    "            \"MSE_NW\": mse_NW,\n",
    "            \"Bias_simple\": bias_simple,\n",
    "            \"MSE_simple\": mse_simple\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki symulacji przedstawiamy w tabeli [-@tbl-sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  $n$    $t_0$     bias_NW     MSE_NW    Bias_simple    MSE_simple\n",
       "-----  -------  ----------  ---------  -------------  ------------\n",
       "   10      0.5   0.0739667  0.305753       0.0854036     0.334649\n",
       "   10      1     0.0576342  0.185804       0.0828919     0.22627\n",
       "   10      2    -0.0501649  0.0800284     -0.0763162     0.0612459\n",
       "   30      0.5   0.023567   0.0925241      0.0254824     0.0954239\n",
       "   30      1     0.0219996  0.0572527      0.0314326     0.0670109\n",
       "   30      2     0.0237012  0.0412222      0.0526667     0.0672316"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Wyniki symulacji dla $\\vartheta = 1$\n",
    "#| label: tbl-sim\n",
    "#| echo: false\n",
    "df_results.columns = [r'$n$', r'$t_0$', r'bias_NW', 'MSE_NW', 'Bias_simple', \"MSE_simple\"]\n",
    "table_stats = tabulate(\n",
    "    df_results,\n",
    "    headers='keys',\n",
    "    tablefmt='latex_booktab',\n",
    "    showindex=False,\n",
    ")\n",
    "\n",
    "Markdown(table_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gdzie \n",
    "$$\n",
    "\\text{MSE} = \\mathbb{E}(X - \\vartheta)^2\n",
    "$$\n",
    "to błąd średniokwadratowy, natomiast\n",
    "$$\n",
    "\\text{Bias} = \\mathbb{E}[X - \\theta]\n",
    "$$\n",
    "oznacza średnie odchylenie estymatora od prawdziwej wartości $\\vartheta$. Natomiast \\_NW oraz \\_simple odnoszą się odpowiednio do estymatora największej wiarygodności oraz do prostszego estymatora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy zauważyć, że:\n",
    "\n",
    "- Estymator największej wiarogodności wykazuje mniejszy bias i niższe MSE w porównaniu do prostszego estymatora.\n",
    "\n",
    "- Wzrost liczby prób $n$ prowadzi do zmniejszenia biasu i MSE dla obu estymatorów.\n",
    "\n",
    "- Wzrost czasu obserwacji $t_0$ powoduje zmniejszenie MSE dla obu estymatorów, natomiast nie ma praktycznie wpływu na bias.\n",
    "\n",
    "- Prostszy estymator daje bardzo dobre wyniki i, mimo pewnej utraty informacji, nie odbiega znacząco od estymatora NW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ilorazu wiarogodności dla danych cenzurowanych I-go typu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozważmy $n$ niezależnych zmiennych losowych $X_1, \\dots, X_n$ o rozkładzie wykładniczym z parametrem $\\vartheta > 0$.  \n",
    "Gęstość tego rozkładu ma postać:\n",
    "$$\n",
    "f_{\\vartheta}(x) = \\vartheta e^{-\\vartheta x} \\cdot \\mathbf{1}_{(0,\\infty)}(x),\n",
    "$$\n",
    "natomiast dystrybuanta:\n",
    "$$\n",
    "F_{\\vartheta}(x) = \\bigl(1 - e^{-\\vartheta x}\\bigr) \\cdot \\mathbf{1}_{(0,\\infty)}(x).\n",
    "$$\n",
    "\n",
    "Zakładamy, że mamy do czynienia z danymi **cenzurowanymi prawostronnie I-go typu** przy czasie cenzurowania $t_0 > 0$.\n",
    "\n",
    "Niech:\n",
    "\n",
    "- $R$ – liczba pełnych obserwacji,  \n",
    "- $S = \\sum_{i=1}^R X_{(i)}$ – suma czasów niecenzurowanych,  \n",
    "- $T_1 = S + (n - R)t_0$ – całkowity czas testowania.\n",
    "\n",
    "Wówczas funkcja wiarogodności dla danych cenzurowanych I-go typu ma postać:\n",
    "$$\n",
    "L(\\vartheta; t^*) \n",
    "= \\frac{n!}{(n - r)!} \\,\n",
    "\\vartheta^r \n",
    "\\exp\\!\\left(\n",
    "    -\\vartheta \\left[ \n",
    "        \\sum_{i=1}^r x_{(i)} + t_0 (n - r) \n",
    "    \\right]\n",
    "\\right),\n",
    "$$\n",
    "gdzie $r$ i $s$ są realizacjami zmiennych losowych $R$ i $S$.\n",
    "\n",
    "Estymator największej wiarogodności parametru $\\vartheta$ ma postać:\n",
    "$$\n",
    "\\hat{\\vartheta} = \\frac{R}{T_1}.\n",
    "$$\n",
    "\n",
    "Zdefiniujmy statystykę:\n",
    "$$\n",
    "\\lambda(R, S) = \n",
    "\\frac{\\displaystyle \\sup_{\\vartheta \\in \\Theta_0} L(\\vartheta; R, S)}\n",
    "     {\\displaystyle \\sup_{\\vartheta \\in \\Theta} L(\\vartheta; R, S)}.\n",
    "$$\n",
    "\n",
    "Zauważmy, że jeśli $\\hat{\\vartheta} \\in \\Theta_0$, to $\\lambda(R, S) = 1$\n",
    "\n",
    "Z twierdzenia Wilksa wynika, że przy założeniu prawdziwości hipotezy zerowej $H_0$ statystyka\n",
    "$$\n",
    "-2 \\ln \\lambda(R, S)\n",
    "$$\n",
    "ma asymptotyczny rozkład $\\chi^2(1)$.\n",
    "\n",
    "Zatem wartość $p$-value  możemy obliczyć jako:\n",
    "$$\n",
    "p\\text{-value} = 1 - F_{\\chi^2(1)}\\!\\bigl(-2 \\ln \\lambda(r, s)\\bigr),\n",
    "$$\n",
    "gdzie $F_{\\chi^2(1)}$ oznacza dystrybuantę rozkładu $\\chi^2$ z jednym stopniem swobody.\n",
    "\n",
    "Hipotezę zerową $H_0$ odrzucamy, jeśli:\n",
    "$$\n",
    "p\\text{-value} < \\alpha,\n",
    "$$\n",
    "gdzie $\\alpha$ to przyjęty poziom istotności.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej przedstawiamy funkcję w języku Python realizującą test ilorazu wiarogodności dla hipotez:\n",
    "\n",
    "- $H_0: \\theta = \\theta_0$ (dwustronny),\n",
    "    \n",
    "- $H_0: \\theta \\leq \\theta_0$ (prawostronny),\n",
    "\n",
    "-  $H_0: \\theta \\geq \\theta_0$ (lewostronny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(r, s, n, t0, theta0, hipoteza):\n",
    "    T1 = (s + (n - r) * t0)\n",
    "    theta_hat = r / T1\n",
    "    L = theta_hat**r * np.exp(-theta_hat*T1)\n",
    "    L0 = theta0**r * np.exp(-theta0*T1)\n",
    "    log_lambda = np.log(L0/L)\n",
    "    if hipoteza == 'dwustronna':\n",
    "        p_value = 1 - scipy.stats.chi2.cdf(-2 * log_lambda, df=1)\n",
    "    \n",
    "    elif hipoteza == 'prawostronna':\n",
    "        if theta_hat <= theta0:\n",
    "            p_value = 1.0\n",
    "        else:\n",
    "            p_value = 1 - scipy.stats.chi2.cdf(-2 * log_lambda, df=1)\n",
    "    \n",
    "    elif hipoteza == 'lewostronna':\n",
    "        if theta_hat >= theta0:\n",
    "            p_value = 1.0\n",
    "        else:\n",
    "            p_value = 1 - scipy.stats.chi2.cdf(-2 * log_lambda, df=1)\n",
    "    \n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symulacyjna estymacja mocy i rozmiaru testu przy wybranych alternatyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przeprowadziliśmy symulacje w celu oszacowania rozmiaru testu oraz jego mocy przy wybranych alternatywach dla hipotezy zerowej $H_0\\!:\\, \\vartheta = \\vartheta_0$.\n",
    "\n",
    "Przyjeliśmy czas eksperymentu $t_0 = 4$, liczności próby $n \\in \\{20, 50\\}$, wartość parametru $\\vartheta_0 = 1/4$ oraz alternatywy\n",
    "$$\n",
    "\\vartheta \\in \\{0.05,\\, 0.1,\\, 0.15,\\, 0.2,\\, 0.25,\\, 0.4,\\, 0.5,\\, 0.7,\\, 0.9,\\, 1.1\\}.\n",
    "$$\n",
    "\n",
    "Dla każdej konfiguracji parametrów postępowaliśmy według następującego algorytmu:\n",
    "\n",
    "- Generujemy $M = 10000$ niezależnych prób z rozkładu wykładniczego $\\mathrm{Exp}(\\vartheta)$, prawostronnie cenzurowanych I-go typu przy czasie eksperymentu $t_0$.\n",
    "\n",
    "- Obliczamy wartości $p$-value z wykorzystaniem wcześniej zdefiniowanego testu.\n",
    "\n",
    "- Na podstawie uzyskanych wartości $p$-value podejmujemy decyzję o odrzuceniu lub braku podstaw do odrzucenia hipotezy zerowej na poziomie istotności $\\alpha = 0.05$.\n",
    "\n",
    "Rozmiar testu oszacowaliśmy jako częstość odrzucenia hipotezy zerowej $H_0$ w symulacjach przeprowadzonych dla $\\vartheta = \\vartheta_0$.\n",
    "\n",
    "Moc testu przy alternatywach oszacowaliśmy analogicznie dla dziesięciu wybranych wartości parametru $\\vartheta \\neq \\vartheta_0$.\n",
    "\n",
    "Ostateczne wyniki symulacji przedstawiliśmy w tabeli [-@tbl-size] oraz na wykresie [-@fig-sim], które ilustrują zależność mocy testu od wartości parametru $\\vartheta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = 0.25\n",
    "t0 = 4\n",
    "n_values = [20, 50]\n",
    "theta_values = [0.08, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "M = 10000\n",
    "alpha = 0.05\n",
    "results = {}\n",
    "size = {}\n",
    "for n in n_values:\n",
    "    power = []\n",
    "    for theta in theta_values:\n",
    "        rejections = 0\n",
    "        for _ in range(M):\n",
    "            X = type_I(t0=t0, L=theta, alpha=1, n=n)\n",
    "            r = np.sum(X.Delta)\n",
    "            s = np.sum(X.Data * X.Delta)\n",
    "            p_val = test(r, s, n, t0, theta0, 'dwustronna')\n",
    "            if p_val < alpha:\n",
    "                rejections += 1       \n",
    "        power.append(rejections / M)\n",
    "        if theta == theta0:\n",
    "            size[n] = rejections / M        \n",
    "    results[n] = power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"Porównanie mocy testu dla $n = 20$ i $n = 50$\"\n",
    "#| fig-align: center\n",
    "#| label: sim\n",
    "#| echo: false\n",
    "\n",
    "df_power = pd.DataFrame(results, index=[f\"{t:.2f}\" for t in theta_values])\n",
    "df_power.index.name = \"Theta\"\n",
    "\n",
    "theta_vals = df_power.index.astype(float)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(theta_vals, df_power[20], marker='o', label='N=20')\n",
    "plt.plot(theta_vals, df_power[50], marker='s', label='N=50')\n",
    "\n",
    "plt.xlabel(r\"$\\vartheta$\", fontsize=12)\n",
    "plt.ylabel(\"Moc testu\", fontsize=12)\n",
    "plt.title(\"Porównanie mocy testu\", fontsize=14)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sim.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  n=20    n=50\n",
       "------  ------\n",
       "0.0517    0.05"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: Rozmiar testu\n",
    "#| label: tbl-size\n",
    "#| echo: false\n",
    "df_size = pd.DataFrame([size])\n",
    "\n",
    "Markdown(tabulate(df_size, headers={\"n=20\",\"n=50\"}, showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Porównanie mocy testu dla $n = 20$ i $n = 50$](sim.pdf){#fig-sim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wykresie [-@fig-sim] widać wyraźną różnicę w mocy testu dla prób o liczności $n=20$ i $n=50$. Test oparty na większej próbie ($n=50$) osiąga wyższe wartości mocy dla tych samych wartości parametru $\\vartheta$, co oznacza, że skuteczniej wykrywa odchylenia od hipotezy zerowej $\\vartheta_0$.\n",
    "\n",
    "Dla obu wielkości próby moc testu jest minimalna w punkcie $\\vartheta_0 = 0.25$, co jest zgodne z oczekiwaniami, w tym punkcie hipoteza zerowa jest prawdziwa, a test powinien ją odrzucać z prawdopodobieństwem zbliżonym do poziomu istotności $\\alpha = 0.05$. Jest to rozmiar naszego testu, a dokładne wartości dla różnych prób przedstawiamy w tabeli [-@tbl-size]. Rzeczywiście, rozmiary testu są bliskie wartości $\\alpha$.\n",
    "\n",
    "Ponadto wykres wskazuje, że moc testu rośnie wraz z oddaleniem wartości $\\vartheta$ od $\\vartheta_0$, co jest zgodne z intuicją: im większa różnica między rozkładami, tym łatwiej testowi odrzucić hipotezę zerową."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weryfikacja hipotezy o średnim czasie do remisji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem analizy jest weryfikacja hipotezy, że średni czas do remisji choroby w grupie A oraz w grupie B wynosi jeden rok. Dane wykorzystane w badaniu są danymi cenzurowanymi I-go typu. Przyjmujemy $t_0 = 1$, ponieważ obserwacje obejmowały okres jednego roku, oraz $\\vartheta_0 = 1$, zgodnie z hipotezą zerową.\n",
    "\n",
    "W analizie zastosowaliśmy test dwustronny przy poziomie istotności $\\alpha = 0.05$.  \n",
    "Poniżej przedstawiamy kod oraz uzyskane wartości $p$-value testu dla obu grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_A = np.sum(delta_A)\n",
    "s_A = np.sum(data_A * delta_A)\n",
    "n_A = len(data_A)\n",
    "t0 = 1\n",
    "theta0 = 1\n",
    "\n",
    "p_val_A = test(r_A, s_A, n_A, t0, theta0, hipoteza='dwustronna')\n",
    "\n",
    "r_B = np.sum(delta_B)\n",
    "s_B = np.sum(data_B * delta_B)\n",
    "n_B = len(data_B)\n",
    "\n",
    "p_val_B = test(r_B, s_B, n_B, t0, theta0, hipoteza='dwustronna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Grupa   |   p-value |\n",
       "|:--------|----------:|\n",
       "| A       |  0.237355 |\n",
       "| B       |  0.323047 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: p-value testów hipotezy $\\vartheta = 1$\n",
    "#| label: tbl-hyp\n",
    "#| echo: false\n",
    "df_pvalues = pd.DataFrame({\n",
    "    \"Grupa\": [\"A\", \"B\"],\n",
    "    \"p-value\": [p_val_A, p_val_B]\n",
    "})\n",
    "\n",
    "Markdown(tabulate(df_pvalues, headers=\"keys\", tablefmt=\"pipe\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z tabeli [-@tbl-hyp] można wywnioskować, że w obu przypadkach brak jest podstaw do odrzucenia hipotezy zerowej, ponieważ wartości $p$-value są większe od przyjętego poziomu istotności $\\alpha$. Oznacza to, że możemy przyjąć hipotezę, że średni czas do remisji choroby w grupie A oraz w grupie B wynosi jeden rok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania dodatkowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alfa: 2, beta: 1, gamma: 2 -> E[X] = 1.1443, sd = 0.4326\n",
      "alfa: 2, beta: 1, gamma: 0.25 -> E[X] = 0.4026, sd = 0.4318\n"
     ]
    }
   ],
   "source": [
    "def ew_mc(alpha, beta, gamma, n=200000):\n",
    "    np.random.seed(37)\n",
    "    u = np.random.rand(n)\n",
    "    samples = np.array([F_inv(ui, alpha, beta, gamma) for ui in u])\n",
    "    mean = samples.mean()\n",
    "    sd = samples.std()\n",
    "    return mean, sd\n",
    "\n",
    "alpha_1, beta_1, gamma_1 = 2, 1, 2\n",
    "\n",
    "alpha_2, beta_2, gamma_2 = 2, 1, 0.25\n",
    "\n",
    "mean_1, sd_1 = ew_mc(alpha_1, beta_1, gamma_1)\n",
    "mean_2, sd_2 = ew_mc(alpha_2, beta_2, gamma_2)\n",
    "\n",
    "print(f\"alfa: {alpha_1}, beta: {beta_1}, gamma: {gamma_1} \\\n",
    "-> E[X] = {mean_1:.4f}, sd = {sd_1:.4f}\")\n",
    "print(f\"alfa: {alpha_2}, beta: {beta_2}, gamma: {gamma_2} \\\n",
    "-> E[X] = {mean_2:.4f}, sd = {sd_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozwijając sumę otrzymujemy\n",
    "$$\n",
    "\\sum_{i=1}^m D_i \n",
    "= \\sum_{i=1}^m (n - i + 1) X_{(i)} \n",
    "- \\sum_{i=1}^m (n - i + 1) X_{(i-1)}.\n",
    "$$\n",
    "Zmieniając zmienną sumowania $j = i - 1$ oraz korzystając z faktu, że $X_{(0)} = 0$, mamy\n",
    "$$\n",
    "\\sum_{i=1}^m D_i \n",
    "= \\sum_{i=1}^m (n - i + 1) X_{(i)} \n",
    "- \\sum_{j=1}^{m-1} (n - j) X_{(j)}.\n",
    "$$\n",
    "Dla $i = 1, \\dots, m-1$ współczynnik przy $X_{(i)}$ wynosi\n",
    "$$\n",
    "(n - i + 1) - (n - i) = 1,\n",
    "$$\n",
    "natomiast dla $i = m$ jest to $n - m + 1$.\n",
    "Zatem\n",
    "$$\n",
    "\\sum_{i=1}^m D_i \n",
    "= \\sum_{i=1}^{m-1} X_{(i)} + (n - m + 1) X_{(m)} \n",
    "= \\sum_{i=1}^m X_{(i)} + (n - m) X_{(m)}.\n",
    "$$\n",
    "Otrzymujemy więc\n",
    "$$\n",
    "T_2 \n",
    "= \\sum_{i=1}^m X_{(i)} + (n - m) X_{(m)} \n",
    "= \\sum_{i=1}^m D_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
